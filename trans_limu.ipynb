{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#@save\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 **kwargs):  # 如果还需要其他参数的话，就用**kwargs返回参数的名称和值\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 40, 512])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"位置编码\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)  # 将它放入到模型当中，但不会更新模型参数，相当于是模型的常数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "x = torch.randn((32, 40, 512))\n",
    "poscode = PositionalEncoding(512, 0.3, 5000)\n",
    "poscode(x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 8])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn.eval()  # 测试模式不改变参数\n",
    "ffn(torch.ones((2, 3, 4))).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[[ 3.0586, -2.4681,  1.3242,  ..., -1.3098,  0.5878,  1.1299],\n",
      "         [-1.1485,  0.4655, -0.9098,  ...,  0.7094, -0.5603,  0.5706],\n",
      "         [ 0.8042, -0.1409,  0.5822,  ...,  0.2066, -0.0838,  1.0853],\n",
      "         ...,\n",
      "         [-0.2655, -1.3634,  0.6959,  ..., -0.7057, -0.1991,  0.7767],\n",
      "         [ 0.1753,  0.0809, -1.8401,  ..., -1.1204, -0.0393,  0.5771],\n",
      "         [ 0.1775, -2.7928, -0.6039,  ..., -0.2735,  1.2567,  1.7595]],\n",
      "\n",
      "        [[-0.0313, -0.1209, -0.5898,  ..., -1.3079,  0.5811,  0.2291],\n",
      "         [-1.7296, -0.8410, -2.0775,  ...,  1.0622,  0.8934, -1.2851],\n",
      "         [ 0.6064,  1.1967,  0.7299,  ...,  0.0940, -0.6343,  1.0663],\n",
      "         ...,\n",
      "         [ 0.5677, -0.1329,  0.4139,  ...,  1.3960,  1.4897, -0.9269],\n",
      "         [ 0.4396,  0.1333,  0.1456,  ..., -0.7347, -1.5791, -0.0551],\n",
      "         [ 0.8407,  0.1423,  0.5619,  ..., -0.6138, -1.8593,  0.7176]],\n",
      "\n",
      "        [[-1.2366,  0.9107,  0.5694,  ...,  1.1684, -2.0010,  0.3816],\n",
      "         [ 0.3020, -0.3475,  0.2089,  ...,  1.2185, -0.0659,  1.0958],\n",
      "         [-1.0330,  0.2129, -0.7804,  ...,  0.2788, -0.5608,  0.5644],\n",
      "         ...,\n",
      "         [-1.8843,  0.2049,  0.5679,  ..., -1.1918,  0.3452, -0.6293],\n",
      "         [-1.0491,  0.3224, -1.2596,  ..., -0.2609, -0.0733, -1.2107],\n",
      "         [-0.4072, -0.2487, -0.6009,  ...,  0.1829,  0.3600,  0.5936]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1359, -0.2514,  0.6871,  ..., -0.7464,  0.6745,  0.4621],\n",
      "         [-0.2328,  2.4942,  1.7804,  ..., -0.3063, -2.0982,  0.3637],\n",
      "         [-0.7795,  0.2668,  0.7981,  ..., -0.4932,  1.2831, -0.6277],\n",
      "         ...,\n",
      "         [ 1.6920, -0.4461,  1.1120,  ..., -1.8281,  0.6735,  1.2774],\n",
      "         [ 0.9509, -0.2948, -1.3840,  ..., -0.3399,  0.5940, -0.2368],\n",
      "         [ 0.6480, -0.7922,  0.9022,  ..., -0.1013,  0.4097,  0.3776]],\n",
      "\n",
      "        [[-1.1256,  1.3456,  0.3359,  ...,  0.2176,  0.6195,  1.3308],\n",
      "         [ 0.0910, -0.7302,  0.8577,  ..., -2.0527,  2.2467,  0.1469],\n",
      "         [-0.9365, -1.5599, -0.9654,  ...,  0.1019,  0.0207, -0.0313],\n",
      "         ...,\n",
      "         [-1.8201, -0.2510,  0.0716,  ...,  0.1225, -0.5959,  0.4913],\n",
      "         [ 0.6337, -0.5724,  0.2559,  ...,  1.0966,  1.4761, -1.1130],\n",
      "         [ 0.1796, -0.9375, -0.7424,  ...,  0.4595,  0.1854, -0.0094]],\n",
      "\n",
      "        [[-1.2261,  0.5671,  0.4520,  ..., -0.9712, -0.0267, -0.6513],\n",
      "         [ 0.3612,  0.5409, -2.1258,  ..., -0.6792, -1.2389, -0.0712],\n",
      "         [-1.2526, -0.1446,  1.2659,  ...,  0.6988, -1.4748, -0.5178],\n",
      "         ...,\n",
      "         [-0.2172, -0.4189, -0.4178,  ..., -0.1446,  0.4542, -1.2586],\n",
      "         [-1.0385,  0.1902, -1.3805,  ..., -1.9854,  0.3933,  0.3610],\n",
      "         [-0.4368,  1.1508, -0.6520,  ..., -0.9075, -1.7162, -0.2470]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[[ 3.1085, -2.6264,  1.3088,  ...,  0.7889, -0.3679, -0.5846],\n",
      "         [-1.3182,  0.6875,  1.7011,  ..., -0.2554, -1.2541,  0.0035],\n",
      "         [ 0.8563,  0.1701, -0.6462,  ...,  0.8569, -0.2575, -0.0660],\n",
      "         ...,\n",
      "         [ 0.1294,  0.1988, -1.8274,  ..., -0.2524, -0.3233, -0.7925],\n",
      "         [-0.4664,  0.5043, -1.3958,  ..., -0.1910, -0.5903,  0.4347],\n",
      "         [-0.7838,  1.5856,  1.3472,  ..., -0.3035,  1.3222,  1.8563]],\n",
      "\n",
      "        [[-0.0882, -0.1799, -0.6597,  ...,  0.2980, -1.0496,  1.0783],\n",
      "         [-0.0083,  0.5042,  0.3086,  ..., -1.7525,  0.1404, -1.2859],\n",
      "         [-1.1245,  0.4678,  1.7849,  ..., -0.6286, -0.2446, -0.2490],\n",
      "         ...,\n",
      "         [ 0.5120, -0.3455, -0.1448,  ..., -1.7223, -1.2392, -1.7504],\n",
      "         [ 0.0922,  0.1466, -0.6973,  ...,  0.6100, -1.0533,  0.4535],\n",
      "         [-0.5851, -0.9346, -0.0359,  ..., -0.6466, -1.9514,  0.7483]],\n",
      "\n",
      "        [[-1.3508,  0.8873,  0.5316,  ..., -1.3023,  0.3637, -0.9673],\n",
      "         [-0.1785, -1.1395, -2.1972,  ...,  0.9661,  0.8639, -1.0480],\n",
      "         [ 0.8013, -0.3136,  0.7095,  ..., -0.3396,  0.0451, -0.4741],\n",
      "         ...,\n",
      "         [ 0.8847, -1.0647, -1.3963,  ...,  0.0276, -0.0742, -0.6830],\n",
      "         [ 1.4517, -0.6895, -0.2524,  ...,  1.8809, -0.1893, -0.1919],\n",
      "         [-0.7347,  0.1925,  0.4543,  ...,  0.1858,  0.3748,  0.6240]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1194, -0.3155,  0.6552,  ..., -0.0191, -0.3322, -0.1600],\n",
      "         [-0.2711, -1.0718, -1.0883,  ..., -0.3334,  0.7140, -2.2572],\n",
      "         [ 0.8156, -1.9686, -0.3407,  ..., -0.8077,  0.3600, -0.3010],\n",
      "         ...,\n",
      "         [ 0.8224,  0.2752,  0.1162,  ..., -0.8243,  1.4295, -0.7144],\n",
      "         [ 1.4857,  1.6549, -0.5159,  ...,  2.2881, -0.0823, -1.9634],\n",
      "         [ 0.4093, -1.0007, -0.1009,  ..., -0.1101,  0.4311,  0.3970]],\n",
      "\n",
      "        [[-1.1998,  1.3315,  0.2973,  ..., -0.9269,  0.6631,  2.0045],\n",
      "         [-1.5000, -0.9043, -1.7446,  ...,  2.6022,  0.3679, -0.3741],\n",
      "         [ 1.4829,  1.3936,  0.4616,  ...,  0.5539, -0.4226,  0.5924],\n",
      "         ...,\n",
      "         [-1.3094, -0.8119,  2.3984,  ...,  0.7456,  0.7971,  1.5205],\n",
      "         [ 2.2461,  1.3539, -1.0361,  ...,  0.1862,  0.9814, -1.5922],\n",
      "         [ 0.3932, -1.0345, -1.3237,  ...,  0.4880,  0.2005, -0.0037]],\n",
      "\n",
      "        [[-1.3208,  0.5187,  0.4006,  ..., -0.0062,  0.1489,  1.2150],\n",
      "         [ 0.2200, -0.6451, -0.0857,  ..., -1.1490, -0.3989, -0.1558],\n",
      "         [ 1.9233,  0.9751, -0.2676,  ...,  0.9473, -0.1863,  0.6772],\n",
      "         ...,\n",
      "         [-1.5449, -0.8590,  0.7062,  ...,  0.1858,  0.2701, -0.2615],\n",
      "         [-0.2093,  0.3917, -1.2165,  ...,  0.4875,  0.2382,  0.0977],\n",
      "         [-1.3411,  0.7939,  1.5966,  ..., -0.9636, -1.8129, -0.2699]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm([10, 512])  # 输入(seq_length, d_model)\n",
    "bn = nn.BatchNorm1d(512)\n",
    "x = torch.randn((32, 10, 512), dtype=torch.float32)\n",
    "print('layer norm:',ln(x), '\\nbatch norm:',bn(x.view(32, 512, 10)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#@save\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化 不改变数据维度格式\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 4])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subsequent_mask(size):\n",
    "    \" sequence mask\"\n",
    "    attn_shape = (1, size, size)\n",
    "    # 返回一个三角矩阵，因为只是标记，所以使用uint8类型的数据，比较简单\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    return subsequent_mask == 0 # 每一个数据都是bool类型的下三角矩阵"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 40, 512])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = math.sqrt(query.size(-1))\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    p_attn = nn.functional.softmax(scores, dim=-1)\n",
    "    if dropout is not None:  # 选择是否使用dropout\n",
    "        p_attn=dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value)\n",
    "test = torch.randn((32, 40, 512))\n",
    "attention(test, test, test, mask=subsequent_mask(40)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 10, 512])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_head, dropout_p=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_head == 0\n",
    "        self.d_k = d_model // num_head\n",
    "        self.num_head = num_head\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 一定要同时创建多个线形层，这样才不一样\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, mask):\n",
    "        batch_size = x_q.size(0)\n",
    "        query = self.W_q(x_q).view(batch_size, -1, self.num_head, self.d_k).transpose(1, 2)  # 先整体计算曾以权重wq，再分头，分头之后各自计算注意力\n",
    "        key = self.W_k(x_k).view(batch_size, -1, self.num_head, self.d_k).transpose(1, 2)\n",
    "        value = self.W_v(x_v).view(batch_size, -1, self.num_head, self.d_k).transpose(1, 2)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # 增加一个维度，且应用到所有的上面\n",
    "        attn_out = attention(query, key, value,mask=mask, dropout=self.dropout)\n",
    "        concat_out = attn_out.view(batch_size, x_q.size(1), -1)  # concat attention\n",
    "        del query\n",
    "        del key\n",
    "        del value  # 清除内存\n",
    "        return self.W_o(concat_out)\n",
    "test = torch.randn((64, 10, 512))\n",
    "mattn = MultiHeadAttention(512, 8, dropout_p=0.2)\n",
    "mattn(test, test, test, mask=subsequent_mask(10)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 10, 512])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoderlayer(nn.Module):\n",
    "    def __init__(self, d_model, norm_size, fnn_size, num_heads, dropout,  **kwargs):\n",
    "        super(Encoderlayer, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_head=num_heads, dropout_p=dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape=norm_size, dropout=dropout)\n",
    "        self.fnn = PositionWiseFFN(ffn_num_input=d_model, ffn_num_hiddens=fnn_size, ffn_num_outputs=d_model)\n",
    "        self.addnorm2 = AddNorm(normalized_shape=norm_size, dropout=dropout)\n",
    "    def forward(self, x, mask):\n",
    "        add_1_out = self.addnorm1(x, self.attention(x, x, x, mask))\n",
    "        return self.addnorm2(add_1_out, self.fnn(add_1_out))\n",
    "test = torch.randn((64, 10, 512))\n",
    "encoder =  Encoderlayer(d_model=512, norm_size=[10, 512], fnn_size=666, num_heads=16, dropout=0.2)\n",
    "encoder(test, subsequent_mask(10)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder(\n",
      "  (embedding): Embedding(43, 512)\n",
      "  (postioncoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (blks): Sequential(\n",
      "    (encoder_block0): Encoderlayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (addnorm1): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fnn): PositionWiseFFN(\n",
      "        (dense1): Linear(in_features=512, out_features=666, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dense2): Linear(in_features=666, out_features=512, bias=True)\n",
      "      )\n",
      "      (addnorm2): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (encoder_block1): Encoderlayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (addnorm1): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fnn): PositionWiseFFN(\n",
      "        (dense1): Linear(in_features=512, out_features=666, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dense2): Linear(in_features=666, out_features=512, bias=True)\n",
      "      )\n",
      "      (addnorm2): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (encoder_block2): Encoderlayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (addnorm1): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fnn): PositionWiseFFN(\n",
      "        (dense1): Linear(in_features=512, out_features=666, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dense2): Linear(in_features=666, out_features=512, bias=True)\n",
      "      )\n",
      "      (addnorm2): AddNorm(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((43, 512), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([64, 43, 512])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,vocab_size, d_model, norm_size, fnn_size, num_heads, dropout_p, num_layers,  **kwargs):\n",
    "        super(encoder, self).__init__(**kwargs)\n",
    "        self.d_model=  d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.postioncoding = PositionalEncoding(d_model=d_model, dropout=dropout_p)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('encoder_block' + str(i),\n",
    "                                 Encoderlayer(d_model=d_model, norm_size=norm_size, fnn_size=fnn_size,\n",
    "                                              num_heads=num_heads, dropout=dropout_p))  # 增加模块的代码\n",
    "\n",
    "    def forward(self, x,mask, *args):\n",
    "        x = self.postioncoding(self.embedding(x) * math.sqrt(self.d_model))\n",
    "        for i ,layer in enumerate(self.blks):\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "encode = encoder(vocab_size=43, d_model=512, norm_size=[43, 512], fnn_size=666, num_heads=8, dropout_p=0.1, num_layers=3)\n",
    "print(encode)\n",
    "test = torch.randint(low=0,high=43, size=(64, 43))\n",
    "encode(test, subsequent_mask(43)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 43, 512])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enbed = nn.Embedding(43, 512)\n",
    "# 使用embedding必须使用longtensor，输入的是索引，这里的43可以指43个不同的类别，每一个类别可以用512维的向量表示，维数越多越容易表示\n",
    "test = torch.randint(low=0,high=43, size=(64, 43))\n",
    "enbed(test).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 40, 512])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class decoderlayer(nn.Module):\n",
    "    def __init__(self,d_model, norm_size, fnn_size, num_heads, dropout, **kwargs):\n",
    "        super(decoderlayer, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(d_model=d_model, num_head=num_heads, dropout_p=dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape=norm_size, dropout=dropout)\n",
    "        self.attention2 = MultiHeadAttention(d_model=d_model, num_head=num_heads, dropout_p=dropout)\n",
    "        self.addnorm2 = AddNorm(normalized_shape=norm_size, dropout=dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input=d_model, ffn_num_hiddens=fnn_size, ffn_num_outputs=d_model)\n",
    "        self.addnorm3 = AddNorm(normalized_shape=norm_size, dropout=dropout)\n",
    "    # 2个注意力计算，一个前馈神经网络\n",
    "    def forward(self, x, kv_memory, src_mask=None, tgt_amsk=None):\n",
    "        \"\"\"\n",
    "        训练阶段，输出序列的所有词元都在同一时间处理，\n",
    "        预测阶段，输出序列是通过词元一个接着一个解码，\n",
    "        \"\"\"\n",
    "        m = kv_memory\n",
    "        x2 = self.attention1(x, x, x, tgt_amsk)\n",
    "        x2 = self.addnorm1(x, x2)\n",
    "        # 编码器－解码器注意力。\n",
    "        x3 = self.attention2(x2, m, m, src_mask)\n",
    "        Z = self.addnorm2(x3, x3)\n",
    "        return self.addnorm3(Z, self.ffn(Z))\n",
    "decoder_unit = decoderlayer(d_model=512, norm_size=[40, 512], fnn_size=485, num_heads=8, dropout=0.3)\n",
    "decoder_unit.eval()\n",
    "test = torch.randn((32, 40, 512))\n",
    "m = torch.randn((32, 40, 512))\n",
    "decoder_unit(test, m).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1280x512 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [78]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     25\u001B[0m test \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, high\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m28\u001B[39m, size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m40\u001B[39m))\n\u001B[0;32m     26\u001B[0m memory \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn((\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m40\u001B[39m, \u001B[38;5;241m512\u001B[39m))\n\u001B[1;32m---> 27\u001B[0m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [78]\u001B[0m, in \u001B[0;36mdecoder.forward\u001B[1;34m(self, x, memory, src_mask, tgt_mask)\u001B[0m\n\u001B[0;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)\u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model))\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblks):\n\u001B[1;32m---> 21\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(x)\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [71]\u001B[0m, in \u001B[0;36mdecoderlayer.forward\u001B[1;34m(self, x, kv_memory, src_mask, tgt_amsk)\u001B[0m\n\u001B[0;32m     18\u001B[0m x2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maddnorm1(x, x2)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# 编码器－解码器注意力。\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m x3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m Z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maddnorm2(x3, x3)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maddnorm3(Z, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mffn(Z))\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [63]\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[1;34m(self, x_q, x_k, x_v, mask)\u001B[0m\n\u001B[0;32m     14\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m x_q\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     15\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_q(x_q)\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_head, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# 先整体计算曾以权重wq，再分头，分头之后各自计算注意力\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW_k\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_k\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_head, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     17\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_v(x_v)\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_head, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1280x512 and 256x256)"
     ]
    }
   ],
   "source": [
    "class decoder(d2l.AttentionDecoder):\n",
    "    def __init__(self,vocab_size, d_model, norm_size, fnn_size, num_heads, dropout_p, num_layers,  **kwargs):\n",
    "        super(decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layer = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model, dropout=0.1)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('decoder_unit'+str(i),\n",
    "                                 decoderlayer(\n",
    "                                    d_model=d_model, norm_size=norm_size,\n",
    "                                     fnn_size=fnn_size, num_heads=num_heads, dropout=dropout_p,\n",
    "\n",
    "                                 ))\n",
    "        self.dense = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
    "        x = self.pos_encoding(self.embedding(x)* math.sqrt(self.d_model))\n",
    "        for i, layer in enumerate(self.blks):\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.dense(x)\n",
    "decoder = decoder(vocab_size=28, d_model=256, norm_size=[40, 256], fnn_size=154, num_heads=8, dropout_p=0.2, num_layers=1)\n",
    "# print(decoder)\n",
    "test = torch.randint(low=0, high=28, size=(32, 40))\n",
    "memory = torch.randn((32, 40, 512))\n",
    "decoder(test, memory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}